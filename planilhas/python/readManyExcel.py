import pandas as pd
import glob
import os
from datetime import datetime

def processar_arquivo_individual(arquivo):
    """
    Processa um √∫nico arquivo CSV, agrupando solicita√ß√µes duplicadas
    e somando seus valores.

    Este √© o primeiro filtro: elimina duplicatas DENTRO do mesmo arquivo.
    """
    try:
        print(f"\n  ‚Üí Lendo: {os.path.basename(arquivo)}")

        # L√™ o arquivo com as configura√ß√µes corretas
        df = pd.read_csv(arquivo, delimiter=';', encoding='utf-8')

        # Normaliza o n√∫mero da solicita√ß√£o para garantir que seja num√©rico
        df['Solicita√ß√£o'] = pd.to_numeric(df['Solicita√ß√£o'], errors='coerce')

        # Normaliza o valor da solicita√ß√£o
        # Importante: valores brasileiros v√™m como "1.234,56" e precisam virar 1234.56
        df['Vl.Solicita√ß√£o'] = (
            df['Vl.Solicita√ß√£o']
            .astype(str)
            .str.replace('.', '', regex=False)  # Remove separador de milhar
            .str.replace(',', '.', regex=False)  # V√≠rgula decimal vira ponto
            .str.strip()
        )
        df['Vl.Solicita√ß√£o'] = pd.to_numeric(df['Vl.Solicita√ß√£o'], errors='coerce')

        # Remove linhas inv√°lidas
        df_limpo = df.dropna(subset=['Solicita√ß√£o'])

        # Agrupa por solicita√ß√£o, somando os valores duplicados
        # Cada solicita√ß√£o pode ter m√∫ltiplos itens, ent√£o somamos os valores
        # mas mantemos apenas o primeiro registro das outras informa√ß√µes
        df_agrupado = df_limpo.groupby('Solicita√ß√£o').agg(
            Empresa=('Empresa', 'first'),
            Data=('Data', 'first'),
            Situacao=('Situa√ß√£o', 'first'),
            Usuario=('Usu√°rio', 'first'),
            Nr_nf=('Nr. Nf', 'first'),
            Sku=('Sku', 'first'),
            Dt_Preventrega=('Dt. Preventrega', 'first'),
            Pedido=('Pedido', 'first'),
            Ds_Prioridade=('Ds. Prioridade', 'first'),
            Ds_Compra=('Ds. Compra', 'first'),
            Vl_Solicitacao_Total=('Vl.Solicita√ß√£o', 'sum'),
            Cod_Ccusto=('Cod. Ccusto', 'first'),
            Obs_lin1=('Obs lin1', 'first'),
            Obs_lin2=('Obs lin2', 'first'),
            Obs_lin3=('Obs lin3', 'first'),
            Obs_lin4=('Obs lin4', 'first')
        ).reset_index()

        # Arredonda para 2 casas decimais
        df_agrupado['Vl_Solicitacao_Total'] = df_agrupado['Vl_Solicitacao_Total'].round(2)

        print(f"     ‚úì {len(df)} linhas ‚Üí {len(df_agrupado)} solicita√ß√µes √∫nicas")

        return df_agrupado

    except Exception as e:
        print(f"     ‚úó Erro ao processar {arquivo}: {e}")
        return None


def consolidar_multiplos_arquivos(padrao_arquivos, arquivo_saida):
    """
    Consolida m√∫ltiplos arquivos CSV em uma √∫nica base, eliminando duplicatas
    entre arquivos (isso resolve o problema de datas sobrepostas).

    Este √© o segundo filtro: elimina duplicatas ENTRE arquivos diferentes.
    Quando a mesma solicita√ß√£o aparecer em m√∫ltiplos arquivos, mantemos
    apenas a vers√£o mais recente (a do √∫ltimo arquivo processado).
    """
    try:
        # Busca todos os arquivos que correspondem ao padr√£o
        arquivos = glob.glob(padrao_arquivos)

        if not arquivos:
            print(f"Erro: Nenhum arquivo encontrado com o padr√£o '{padrao_arquivos}'")
            return None

        # Ordena os arquivos por nome (assumindo que t√™m data no nome)
        # Isso garante que processamos do mais antigo para o mais recente
        arquivos.sort()

        print(f"\n{'='*60}")
        print(f"CONSOLIDANDO {len(arquivos)} ARQUIVO(S)")
        print(f"{'='*60}")

        lista_dataframes = []

        # Processa cada arquivo individualmente
        for arquivo in arquivos:
            df_processado = processar_arquivo_individual(arquivo)
            if df_processado is not None:
                lista_dataframes.append(df_processado)

        if not lista_dataframes:
            print("Erro: Nenhum arquivo foi processado com sucesso")
            return None

        print(f"\n{'='*60}")
        print("ELIMINANDO DUPLICATAS ENTRE ARQUIVOS")
        print(f"{'='*60}")

        # Junta todos os dataframes em um s√≥
        df_completo = pd.concat(lista_dataframes, ignore_index=True)
        print(f"  Total de linhas antes de remover duplicatas: {len(df_completo)}")

        # Remove duplicatas mantendo a √∫ltima ocorr√™ncia
        # Isso √© crucial: se a solicita√ß√£o 12345 aparece no arquivo de 20/10
        # e tamb√©m no arquivo de 27/10, mantemos a do arquivo de 27/10 (mais recente)
        df_final = df_completo.drop_duplicates(subset=['Solicita√ß√£o'], keep='last')
        print(f"  Total de linhas ap√≥s remover duplicatas: {len(df_final)}")
        print(f"  ‚Üí Foram eliminadas {len(df_completo) - len(df_final)} solicita√ß√µes duplicadas")

        # Ordena por n√∫mero de solicita√ß√£o para facilitar consultas futuras
        df_final = df_final.sort_values('Solicita√ß√£o').reset_index(drop=True)

        # Reordena as colunas
        colunas_ordenadas = [
            'Empresa', 'Data', 'Situacao', 'Usuario', 'Solicita√ß√£o',
            'Nr_nf', 'Sku', 'Dt_Preventrega', 'Pedido', 'Ds_Prioridade',
            'Ds_Compra', 'Vl_Solicitacao_Total', 'Cod_Ccusto',
            'Obs_lin1', 'Obs_lin2', 'Obs_lin3', 'Obs_lin4'
        ]
        df_final = df_final[colunas_ordenadas]

        # Salva o resultado
        df_final.to_csv(arquivo_saida, index=False, sep=';', encoding='utf-8')

        print(f"\n{'='*60}")
        print(f"ARQUIVO FINAL SALVO: {arquivo_saida}")
        print(f"Total de solicita√ß√µes √∫nicas: {len(df_final)}")
        print(f"{'='*60}\n")

        return df_final

    except Exception as e:
        print(f"Erro ao consolidar arquivos: {e}")
        return None


def adicionar_novos_dados_semanais(arquivo_base, padrao_novos_arquivos, arquivo_saida):
    """
    Adiciona novos dados semanais a uma base existente.

    Use esta fun√ß√£o quando voc√™ j√° tem uma base consolidada e quer adicionar
    dados da semana seguinte. A fun√ß√£o garante que n√£o haver√° duplicatas
    mesmo se houver sobreposi√ß√£o de datas.
    """
    try:
        print(f"\n{'='*60}")
        print("ATUALIZANDO BASE EXISTENTE COM NOVOS DADOS")
        print(f"{'='*60}")

        # L√™ a base existente
        print(f"\n  ‚Üí Carregando base existente: {arquivo_base}")
        df_base = pd.read_csv(arquivo_base, delimiter=';', encoding='utf-8')
        print(f"     ‚úì Base tem {len(df_base)} solicita√ß√µes")

        # Processa os novos arquivos
        arquivos_novos = glob.glob(padrao_novos_arquivos)

        if not arquivos_novos:
            print(f"Erro: Nenhum arquivo novo encontrado com o padr√£o '{padrao_novos_arquivos}'")
            return None

        arquivos_novos.sort()
        print(f"\n  ‚Üí Processando {len(arquivos_novos)} arquivo(s) novo(s)")

        lista_novos = []
        for arquivo in arquivos_novos:
            df_processado = processar_arquivo_individual(arquivo)
            if df_processado is not None:
                lista_novos.append(df_processado)

        if not lista_novos:
            print("Erro: Nenhum arquivo novo foi processado com sucesso")
            return None

        # Consolida os novos arquivos (elimina duplicatas entre eles)
        df_novos = pd.concat(lista_novos, ignore_index=True)
        df_novos = df_novos.drop_duplicates(subset=['Solicita√ß√£o'], keep='last')
        print(f"     ‚úì Total de solicita√ß√µes novas/atualizadas: {len(df_novos)}")

        # Junta base antiga com dados novos
        df_completo = pd.concat([df_base, df_novos], ignore_index=True)

        # Remove duplicatas mantendo sempre a vers√£o mais recente (keep='last')
        # Isso garante que se uma solicita√ß√£o j√° existia, ela ser√° atualizada
        df_final = df_completo.drop_duplicates(subset=['Solicita√ß√£o'], keep='last')

        print(f"\n  ‚Üí Base anterior: {len(df_base)} solicita√ß√µes")
        print(f"  ‚Üí Dados novos: {len(df_novos)} solicita√ß√µes")
        print(f"  ‚Üí Base final: {len(df_final)} solicita√ß√µes")
        print(f"  ‚Üí Novas solicita√ß√µes adicionadas: {len(df_final) - len(df_base)}")

        # Ordena e salva
        df_final = df_final.sort_values('Solicita√ß√£o').reset_index(drop=True)
        df_final.to_csv(arquivo_saida, index=False, sep=';', encoding='utf-8')

        print(f"\n{'='*60}")
        print(f"BASE ATUALIZADA SALVA: {arquivo_saida}")
        print(f"{'='*60}\n")

        return df_final

    except Exception as e:
        print(f"Erro ao adicionar novos dados: {e}")
        return None


# ==================== EXEMPLOS DE USO ====================

# CEN√ÅRIO 1: Primeira vez - processar m√∫ltiplos arquivos hist√≥ricos
# Use quando estiver come√ßando e tiver v√°rios arquivos para consolidar
print("\n" + "üî∑" * 30)
print("CEN√ÅRIO 1: CONSOLIDA√á√ÉO INICIAL")
print("üî∑" * 30)
"""
# Exemplo: voc√™ tem arquivos de diferentes semanas na pasta planilhas/csv/
# Todos seguem o padr√£o RICARDOALMEIDA*.csv
resultado = consolidar_multiplos_arquivos(
    padrao_arquivos='planilhas/csv/planilhas_semanais/*/RICARDOALMEIDA*.csv',
    arquivo_saida='planilhas/csv/planilhas_relatorios/relatorio_ate_27-10-2025.csv'
)
"""
# CEN√ÅRIO 2: Atualiza√ß√µes semanais
print("\n" + "üî∂" * 30)
print("CEN√ÅRIO 2: ATUALIZA√á√ÉO SEMANAL (EXEMPLO)")
print("üî∂" * 30)

# Atualiza√ß√£o semanal
resultado_atualizado = adicionar_novos_dados_semanais(
    arquivo_base='planilhas/csv/planilha_geral/planilha_geral_ate_20-10-2025.csv',
    padrao_novos_arquivos='planilhas/csv/planilhas_semanais/*/RICARDOALMEIDA*.csv',
    arquivo_saida='planilhas/csv/planilhas_relatorios/relatorio_ate_27-10-2025.csv'
)

